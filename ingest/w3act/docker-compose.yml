version: '3.2'

services:

  # NGINX to pull all services together:
  nginx:
    image: nginx:alpine
    volumes:
      - ./config/nginx.conf:/etc/nginx/conf.d/default.conf:ro
      - ${W3ACT_STORAGE}/static:/host/static
    ports:
      - "9000:80"

  # PyWB for QA
  pywb:
    image: ukwa/ukwa-pywb:2.6.7.2
    environment:
      UKWA_INDEX: "http://cdx.api.wa.bl.uk/data-heritrix?url={url}&closest={closest}&sort=closest&filter=!statuscode:429&filter=!mimetype:warc/revisit"
      UKWA_ARCHIVE: "webhdfs://warc-server:8000/by-filename/"
      WEBHDFS_USER: "access"
    ports:
      - "7070:8080"
    volumes:
      - ./config/pywb.yaml:/webarchive/config.yaml
    deploy:
      # If we don't place a limit on resources it just consumes all RAM rather than cleaning up:
      resources:
        limits:
          cpus: '4.0'
          memory: '2G'


  # Internal WARC server that can pull from multiple clusters and local disk
  warc-server:
    image: ukwa/warc-server:1.1.0
    # Modify number of workers:
    command: "gunicorn --log-level warn --error-logfile - --access-logfile - --bind 0.0.0.0:8000 --workers 2 --worker-class gthread --threads 25 warc_server:app"
    # Alternative command for debugging:
    #command: "gunicorn --log-level debug --error-logfile - --access-logfile - --bind 0.0.0.0:8000 --workers 5 --worker-class gthread --threads 20 warc_server:app"
    ports:
      - 8001:8000
    environment:
      - "WARC_PATHS=/heritrix/output,/heritrix/wren"
      - "TRACKDB_URL=http://solr8.api.wa.bl.uk/solr/tracking"
      # Alternative TrackDB for testing:
      #- "TRACKDB_URL=http://trackdb.dapi.wa.bl.uk/solr/tracking"
      #- "TRACKDB_FETCH_LIMIT=100000"
      # Default WebHDFS service used if there's no access_url_s in TrackDB (user is 'access'):
      - "WEBHDFS_PREFIX=http://hdfs.api.wa.bl.uk/webhdfs/v1"
    volumes:
      - "/mnt/gluster/fc/heritrix/output:/heritrix/output"
      - "/mnt/gluster/fc/heritrix/wren:/heritrix/wren"

  # PDF to HTML service
  pdf2htmlex:
    image: ukwa/pdf2htmlex:1.0.1
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: '2G'
    depends_on:
      - pywb

  # W3ACT
  w3act:
    image: ukwa/w3act:2.3.5
    #image: ukwa/w3act:master
    command: /w3act/bin/w3act -J-Xmx8g -J-XX:+ExitOnOutOfMemoryError -Dconfig.file=/w3act/conf/docker.conf -Dpidfile.path=/dev/null -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=1898 -Dcom.sun.management.jmxremote.rmi.port=1898 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.authenticate=false -Djava.rmi.server.hostname=${JMX_HOSTNAME}
    environment:
      - "APPLICATION_NAVBAR_COLOR=${APPLICATION_NAVBAR_COLOR}"
      - "APPLICATION_SECRET=${APPLICATION_SECRET}"
      - "SECRET_SERVER_USER=${SECRET_SERVER_USER}"
      - "SECRET_SERVER_PASSWORD=${SECRET_SERVER_PASSWORD}"
      - "DB_DRIVER=org.postgresql.Driver"
      - "DB_URI=postgres://w3act:${W3ACT_PSQL_PASSWORD}@postgres/w3act"
      - "USE_TEST_DATA=false"
      - "ENABLE_DDHAPT=true"
      - "SMTP_SERVER=juno.bl.uk"
      - "WAYBACK_URL=http://pywb:8080/archive/" # Used by PDF2HTML proxy
      - "CDXSERVER_ENDPOINT=http://cdx.api.wa.bl.uk/data-heritrix"
      - "MONITRIX_URI=http://monitrix:5601/app/kibana"
      - "PDFTOHTMLEX_URI=http://pdf2htmlex:5000/convert?url="
      - "AMQP_HOST=amqp"
      - "SERVER_NAME=${W3ACT_SERVER_NAME}"
      - "PII_URI=${PII_URL}"
      - "ACCESS_RESOLVER_URI=${W3ACT_SERVER_NAME}/access/resolve/"
      - "ENABLE_EVOLUTIONS=true"
      - "APPLY_EVOLUTIONS=true" # This is generally safe to enable, but should only be done when migrations are expected.
      - "APPLY_DOWN_EVOLUTIONS=false" # WARNING DOWNS ARE POTENTIALLY DESTRUCTIVE
    volumes:
      - "${DDHAPT_SIPS_SUBMITTED_DIR}:/opt/data/dls_sips_copy"
      - "${DDHAPT_EBOOKS_SUBMISSION_DIR}:/opt/data/w3act/ebooks:shared"
      - "${DDHAPT_EJOURNALS_SUBMISSION_DIR}:/opt/data/w3act/ejournals:shared"
    ports:
      - "9001:9000"
      - "1898:1898"
    depends_on:
      - pywb
      - pdf2htmlex

  # PostgreSQL
  postgres:
    image: postgres:9.6.2
    environment:
      - "POSTGRES_USER=w3act"
      - "POSTGRES_PASSWORD=${W3ACT_PSQL_PASSWORD}"
    ports:
      - "5432:5432"
    volumes:
      - "${W3ACT_PSQL_DIR}:/var/lib/postgresql/data"
      - "${W3ACT_DUMPS_DIR}:/tmp"

  # -------------------------------------------------------------
  # Additional database for DDHAPT
  # -------------------------------------------------------------
  ddhapt-postgres:
    image: postgres:14.1
    environment:
      - "POSTGRES_USER=ddhapt"
      - "POSTGRES_PASSWORD=${DDHAPT_PSQL_PASSWORD}"
    ports:
      - "5435:5432"
    volumes:
      - "${DDHAPT_PSQL_DIR}:/var/lib/postgresql/data"

  # -------------------------------------------------------------
  # Crawl log viewer
  # -------------------------------------------------------------
  crawl-log-viewer:
    image: ukwa/crawl-log-viewer
    environment:
      - "SCRIPT_NAME=/act/logs"
    volumes:
      - ./config/crawl-log-viewer-topics.json:/usr/src/app/topics.json

  # -------------------------------------------------------------
  # Notebooks and Dashboards
  # -------------------------------------------------------------
  nbapps:
    image: ukwa/ukwa-notebook-apps:master
    # Make sure idle kernels get shut down (or we will run out of memory!):
    command: "--MappingKernelManager.cull_interval=60 --MappingKernelManager.cull_idle_timeout=120 --base_url=/act/nbapps/ --no-browser --Voila.ip=0.0.0.0 --debug /app"
    environment:
     - "HTTPS_PROXY=http://explorer.bl.uk:3127" # This is needed because we're talking to the live public API (we should have an internal one)
     - "PYTHONPATH=/app"
     - "API_PREFIX=https://${SERVER_NAME}"
     - "WB_PREFIX=https://${SERVER_NAME}"
     - "TRACKDB_LIST_JSONL=/storage/trackdb_list.jsonl"
     - "AWS_S3_LIST_JSONL=/storage/aws_s3_list.jsonl"
    ports:
      - 8866:8866
    volumes:
      - "${STORAGE_PATH}:/storage"

  grafana:
    image: grafana/grafana-oss:8.3.4
    environment:
      - "GF_SERVER_ROOT_URL=%(protocol)s://%(domain)s:%(http_port)s/act/grafana/"
      - "GF_LIVE_ALLOWED_ORIGINS=https://*.webarchive.org.uk"
      - "GF_SERVER_SERVE_FROM_SUB_PATH=true"
      - "GF_SECURITY_ADMIN_USER=admin" # Auth against W3ACT handled in NGINX
      - "GF_AUTH_PROXY_ENABLED=true"
      - "GF_AUTH_PROXY_HEADER_NAME=X-AUTH-REMOTE-USER-EMAIL" 
      # Consider adding X-Auth-Remote-User-Primary-Role as a Group 
      - "GF_AUTH_PROXY_HEADER_PROPERTY=email"
      - "GF_AUTH_PROXY_AUTO_SIGN_UP=true"
      - "GF_AUTH_DISABLE_SIGNOUT_MENU=true"
      - "HTTPS_PROXY=${HTTPS_PROXY}"
      - "W3ACT_PSQL_VIEWER_PASSWORD=${W3ACT_PSQL_VIEWER_PASSWORD}"
    volumes:
      - ./grafana:/etc/grafana/provisioning
      - "${W3ACT_STORAGE}/grafana:/var/lib/grafana"
      - "${W3ACT_STORAGE}/dbs:/dbs"
    ports:
      - 3001:3000

  # -------------------------------------------------------------
  # SolrWayback service for Staff
  # -------------------------------------------------------------
  solrwayback:
    image: ukwa/solrwayback:master
    ports:
      - "18081:8080"
    environment: 
      # Allow Solr and WARC server locations to be overidden:
      - "SOLR_URL=http://solr.api.wa.bl.uk/solr/all"
      - "WARC_SERVER_PREFIX=http://warc-server:8000/by-filename/"
      - "BASE_URL=https://${SERVER_NAME}/act/solrwayback/"
      - "ALT_WAR_NAME=act#solrwayback"
      - "ALT_PLAYBACK_PREFIX=https://${SERVER_NAME}/act/wayback/archive/"

  # -------------------------------------------------------------
  # Backstage Browser
  # -------------------------------------------------------------
  backstage:
    image: ukwa/backstage:master
    environment:
     - "SOLR_URL=http://solr.api.wa.bl.uk/solr/all"
     - "DEPLOY_RELATIVE_URL_ROOT=/act/backstage"
     - "RAILS_SERVE_STATIC_FILES=true"
    ports:
     - 3002:3000

networks:
  # Make network attachable so tasks can be run:
  default:
    driver: overlay
    attachable: true

